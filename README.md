# Open-Code-Zero
<div align="center" style="line-height: 1;">
  <a href="https://nebula-petunia-008.notion.site/Aha-Moment-in-Code-Reasoning-from-7B-Base-Models-and-15k-data-1b138625ace8802a9740de5b6fe567ed" target="_blank">
  <img alt="Notion Page"
    src="https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white"/></a>
  <br>
</div>

Welcome to the official repo for Open-Code-Zero! We pioneered replicating Deepseel-R1-Zeroâ€™s self-reflective reasoning in *pure code* using minimalist RL (no math data!) on a 7B coder model. Key discoveries:

- ðŸš€ **Emergent Long-COT**: Achieved sophisticated self-correction with just 15k problems and ~600 training steps.
- ðŸ§  **System-2 Awakening**: Chaotic "quick thinking" gives way to structured, critical analysis as training stabilizes.
- ðŸ’» **Coder Advantage**: Code-specialized models avoid language-switching instability seen in general LLMs, enabling cleaner reasoning.
- ðŸ«¢Â **First** to prove code-domain LLMs can intrinsically evolve Deepseek-R1-Zero-style reasoning *without math*. Dive in for paradigm-shifting examples!

# Training Settings & Code




We find by using a simple outcome-based reward, model learns to naturally adopt a more sophisticated reasoning pattern gradually during training
Coming soon in just a few days! Stay tuned and star our repo if you are interested :) 
